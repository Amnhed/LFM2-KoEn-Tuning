{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ GRPO v8: ê°•í™”í•™ìŠµ ë²ˆì—­ ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "LFM2-1.2B ëª¨ë¸ì„ COMET ë³´ìƒ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ GRPOë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "> **í™˜ê²½**: Google Colab T4 GPU\n",
    "\n",
    "## ì‚¬ìš© ì „ ì„¤ì •\n",
    "1. Colab Secretsì— `HF_TOKEN` ë“±ë¡ (ê¶Œì¥)\n",
    "2. ë°ì´í„°ì…‹ íŒŒì¼ ì—…ë¡œë“œ (`dataset_rl_final_10k.jsonl`)\n",
    "2. `OUTPUT_MODEL_ID`ë¥¼ ë³¸ì¸ HuggingFace IDë¡œ ë³€ê²½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip uninstall -y numpy\n",
    "!pip install \"numpy>=1.26.0,<2.0.0\"\n",
    "!pip install -q transformers peft accelerate bitsandbytes\n",
    "!pip install -q unbabel-comet datasets\n",
    "!pip install -q git+https://github.com/huggingface/trl.git\n",
    "\n",
    "print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ. [ëŸ°íƒ€ì„] -> [ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘] í´ë¦­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. HuggingFace ë¡œê·¸ì¸\n",
    "from huggingface_hub import login\n",
    "login()  # Colab Secrets ì‚¬ìš© ë˜ëŠ” ìˆ˜ë™ ì…ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback\n",
    ")\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ğŸ”§ CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ì„¤ì • (âš ï¸ OUTPUT_MODEL_IDëŠ” ë³¸ì¸ IDë¡œ ë³€ê²½!)\n",
    "MODEL_ID = \"gyung/lfm2-1.2b-koen-mt-v6.4-merged\"\n",
    "OUTPUT_MODEL_ID = \"YOUR_ID/your-output-model\"  # âš ï¸ ë³€ê²½ í•„ìš”!\n",
    "DATASET_REPO = \"gyung/lfm2-koen-samples\"\n",
    "DATASET_CONFIG = \"manual_1000_grpo\"  # GRPOìš© ë°ì´í„°ì…‹\n",
    "\n",
    "print(f\"ğŸ“Œ Base Model: {MODEL_ID}\")\n",
    "print(f\"ğŸ“Œ Dataset: {DATASET_REPO} ({DATASET_CONFIG})\")\n",
    "print(f\"ğŸ“Œ Dataset: {DATASET_REPO} ({DATASET_CONFIG})\")\n",
    "print(f\"ğŸ“Œ Output: {OUTPUT_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. COMET ì‹¬ì‚¬ ëª¨ë¸ ë¡œë“œ\n",
    "print(\"âš–ï¸ ì‹¬ì‚¬ìœ„ì›(COMET) ë¡œë”© ì¤‘...\")\n",
    "comet_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_path)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        comet_model = comet_model.to(\"cuda\")\n",
    "        print(\"âœ… COMET ëª¨ë¸ GPU ë¡œë“œ ì™„ë£Œ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ COMET GPU ë¡œë“œ ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ë°ì´í„°ì…‹ ì¤€ë¹„ (HuggingFaceì—ì„œ ë¡œë“œ)\n",
    "print(f\"ğŸ“š ë°ì´í„°ì…‹ ë¡œë”©: {DATASET_REPO} ({DATASET_CONFIG})\")\n",
    "dataset = load_dataset(DATASET_REPO, DATASET_CONFIG, split=\"train\")\n",
    "print(f\"   ìƒ˜í”Œ ìˆ˜ (ì–‘ë°©í–¥ í¬í•¨): {len(dataset)}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "def create_bidirectional_prompts(batch):\n",
    "    \"\"\"\n",
    "    í•˜ë‚˜ì˜ ìŒ(Input/Output)ì„ ë‘ ê°œì˜ í•™ìŠµ ë°ì´í„°ë¡œ ë³€í™˜\n",
    "    1. En -> Ko\n",
    "    2. Ko -> En\n",
    "    \"\"\"\n",
    "    new_prompts = []\n",
    "    new_srcs = []\n",
    "    new_refs = []\n",
    "\n",
    "    for en, ko in zip(batch['input'], batch['output']):\n",
    "        # 1. En -> Ko\n",
    "        msgs_en_ko = [\n",
    "            {\"role\": \"system\", \"content\": \"Translate the following text to Korean.\"},\n",
    "            {\"role\": \"user\", \"content\": en}\n",
    "        ]\n",
    "        prompt_en = tokenizer.apply_chat_template(msgs_en_ko, tokenize=False, add_generation_prompt=True)\n",
    "        new_prompts.append(prompt_en)\n",
    "        new_srcs.append(en)\n",
    "        new_refs.append(ko)\n",
    "\n",
    "        # 2. Ko -> En\n",
    "        msgs_ko_en = [\n",
    "            {\"role\": \"system\", \"content\": \"Translate the following text to English.\"},\n",
    "            {\"role\": \"user\", \"content\": ko}\n",
    "        ]\n",
    "        prompt_ko = tokenizer.apply_chat_template(msgs_ko_en, tokenize=False, add_generation_prompt=True)\n",
    "        new_prompts.append(prompt_ko)\n",
    "        new_srcs.append(ko)\n",
    "        new_refs.append(en)\n",
    "\n",
    "    return {\"prompt\": new_prompts, \"src\": new_srcs, \"ref\": new_refs}\n",
    "\n",
    "dataset = dataset.map(create_bidirectional_prompts, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "print(f\"âœ… ì–‘ë°©í–¥ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(f\"   ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(dataset)} (ì›ë³¸ì˜ 2ë°°)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ë³´ìƒ í•¨ìˆ˜ ì •ì˜ (COMET ê¸°ë°˜)\n",
    "def reward_function(completions, src, ref, **kwargs):\n",
    "    \"\"\"\n",
    "    COMET ê¸°ë°˜ ë³´ìƒ í•¨ìˆ˜\n",
    "    TRL ìµœì‹  API: datasetì˜ ì»¬ëŸ¼ëª…(src, ref)ì´ í‚¤ì›Œë“œ ì¸ìë¡œ ì „ë‹¬ë¨\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    for s, mt, r in zip(src, completions, ref):\n",
    "        # completionsê°€ dictì¸ ê²½ìš° (conversational format)\n",
    "        if isinstance(mt, dict):\n",
    "            mt_text = mt.get('content', str(mt))\n",
    "        elif isinstance(mt, list) and len(mt) > 0:\n",
    "            mt_text = mt[-1].get('content', str(mt[-1])) if isinstance(mt[-1], dict) else str(mt[-1])\n",
    "        else:\n",
    "            mt_text = str(mt)\n",
    "\n",
    "        inputs.append({\"src\": s, \"mt\": mt_text, \"ref\": r})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gpus = 1 if torch.cuda.is_available() else 0\n",
    "        model_output = comet_model.predict(inputs, batch_size=8, gpus=gpus)\n",
    "\n",
    "    return model_output.scores\n",
    "\n",
    "print(\"âœ… ë³´ìƒ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)\n",
    "print(\"ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "# Norm ë ˆì´ì–´ ì •ë°€ë„ ì¡°ì •\n",
    "for name, module in model.named_modules():\n",
    "    if \"norm\" in name.lower():\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "model.config.torch_dtype = torch.float16\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. LoRA ë° í•™ìŠµ ì„¤ì •\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=\"all-linear\",\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"./lfm2-rl-final\",\n",
    "\n",
    "    # í•™ìŠµ ì„¤ì •\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "\n",
    "    # GRPO ì„¤ì •\n",
    "    num_generations=4,\n",
    "    max_completion_length=200,\n",
    "    beta=0.04,\n",
    "    use_vllm=False,\n",
    "\n",
    "    # ì •ë°€ë„\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "\n",
    "    # ë¡œê¹… ë° ì €ì¥\n",
    "    logging_steps=1,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"steps\",\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # Hub ì—…ë¡œë“œ\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=OUTPUT_MODEL_ID,\n",
    "    hub_strategy=\"every_save\",\n",
    ")\n",
    "\n",
    "print(\"âœ… í•™ìŠµ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Trainer ìƒì„± ë° í•™ìŠµ\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=reward_function,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ\n",
    "trainer.save_model(\"./final_model\")\n",
    "trainer.push_to_hub()\n",
    "print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {OUTPUT_MODEL_ID}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}