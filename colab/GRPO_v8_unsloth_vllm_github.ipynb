{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ GRPO v8: Unsloth + vLLM Optimized (LFM2-KoEn)\n",
        "\n",
        "**Korean-English Bidirectional Translation Tuning with GRPO**\n",
        "\n",
        "- **Base Model**: `gyung/lfm2-1.2b-koen-mt-v6.4-merged`\n",
        "- **Optimization**: Unsloth + vLLM (for 2x-5x faster generation)\n",
        "- **Direction**: Bidirectional (En<->Ko)\n",
        "\n",
        "### âš ï¸ Critical Notes\n",
        "1. T4 GPU (16GB VRAM)ì—ì„œëŠ” ë©”ëª¨ë¦¬ ê´€ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
        "2. `vllm_gpu_memory_utilization = 0.5` ê¶Œì¥\n",
        "3. ëŸ°íƒ€ì„ ì¬ì‹œì‘ì´ í•„ìˆ˜ì…ë‹ˆë‹¤ (ì„¤ì¹˜ í›„)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ëŸ°íƒ€ì„ ì¬ì‹œì‘ í•„ìš”)\n",
        "!pip uninstall -y numpy\n",
        "!pip install unsloth vllm\n",
        "!pip install -U git+https://github.com/huggingface/trl.git\n",
        "!pip install peft accelerate bitsandbytes unbabel-comet\n",
        "!pip install \"numpy<2.0\"\n",
        "\n",
        "print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ. [ëŸ°íƒ€ì„] -> [ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘] í´ë¦­\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. HuggingFace ë¡œê·¸ì¸\n",
        "from huggingface_hub import login\n",
        "login()  # Colab Secrets ì‚¬ìš© ë˜ëŠ” ìˆ˜ë™ ì…ë ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. GPU ê°ì§€ ë° ì„¤ì •\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "except Exception:\n",
        "    gpu_name = \"CPU\"\n",
        "print(f\"Detected GPU: {gpu_name}\")\n",
        "\n",
        "if \"A100\" in gpu_name:\n",
        "    BATCH_SIZE = 16\n",
        "    GRAD_ACCUM = 2\n",
        "    VLLM_MEM = 0.7\n",
        "elif \"L4\" in gpu_name:\n",
        "    BATCH_SIZE = 8\n",
        "    GRAD_ACCUM = 4\n",
        "    VLLM_MEM = 0.6\n",
        "else:  # T4 (Free Colab) or others\n",
        "    BATCH_SIZE = 4\n",
        "    GRAD_ACCUM = 8\n",
        "    VLLM_MEM = 0.5\n",
        "\n",
        "print(f\"Configured: Batch={BATCH_SIZE}, Accum={GRAD_ACCUM}, vLLM Mem={VLLM_MEM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Unsloth ëª¨ë¸ ë¡œë“œ\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "MODEL_ID = \"gyung/lfm2-1.2b-koen-mt-v6.4-merged\"  # Base ëª¨ë¸\n",
        "OUTPUT_MODEL_ID = \"YOUR_ID/your-output-model\"     # âš ï¸ ë³€ê²½ í•„ìš”!\n",
        "print(f\"ğŸ“Œ Base Model: {MODEL_ID}\")\n",
        "\n",
        "max_seq_length = 512\n",
        "lora_rank = 32\n",
        "\n",
        "# T4ëŠ” FP16 ì‚¬ìš©\n",
        "gpu_name_short = torch.cuda.get_device_name(0)\n",
        "if \"T4\" in gpu_name_short:\n",
        "    dtype = torch.float16\n",
        "    print(\"âš ï¸ T4 Detected: Forcing dtype=float16\")\n",
        "else:\n",
        "    dtype = torch.float16 if not is_bfloat16_supported() else None\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_ID,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# LoRA ì„¤ì •\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=\"all-linear\",\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "print(\"âœ… Unsloth ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. COMET ë³´ìƒ ëª¨ë¸ ë¡œë“œ\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ COMET ì„¤ì¹˜ ì¤‘...\")\n",
        "    !pip install -q unbabel-comet\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "\n",
        "print(\"âš–ï¸ COMET ë¡œë”©...\")\n",
        "comet_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "comet_model = load_from_checkpoint(comet_path)\n",
        "comet_model.eval()\n",
        "\n",
        "COMET_ON_GPU = True\n",
        "if torch.cuda.is_available() and COMET_ON_GPU:\n",
        "    try:\n",
        "        comet_model.to(\"cuda\")\n",
        "        print(\"âœ… COMET on GPU (Enabled)\")\n",
        "    except RuntimeError:\n",
        "        print(\"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, COMET CPU ì‚¬ìš©\")\n",
        "        comet_model.to(\"cpu\")\n",
        "else:\n",
        "    comet_model.to(\"cpu\")\n",
        "\n",
        "def reward_function(completions, src, ref, **kwargs):\n",
        "    inputs = []\n",
        "    for s, mt, r in zip(src, completions, ref):\n",
        "        if isinstance(mt, dict):\n",
        "            mt_text = mt.get('content', str(mt))\n",
        "        elif isinstance(mt, list) and len(mt) > 0:\n",
        "            mt_text = mt[-1].get('content', str(mt[-1])) if isinstance(mt[-1], dict) else str(mt[-1])\n",
        "        else:\n",
        "            mt_text = str(mt)\n",
        "        inputs.append({\"src\": s, \"mt\": mt_text, \"ref\": r})\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gpus = 1 if torch.cuda.is_available() and COMET_ON_GPU else 0\n",
        "        model_output = comet_model.predict(inputs, batch_size=len(inputs), gpus=gpus)\n",
        "\n",
        "    return model_output.scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. ë°ì´í„°ì…‹ ì¤€ë¹„\n",
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_REPO = \"gyung/lfm2-koen-samples\"\n",
        "DATASET_CONFIG = \"manual_1000_grpo\"  # HuggingFace ë°ì´í„°ì…‹\n",
        "# ë¡œì»¬ íŒŒì¼ ì‚¬ìš© ì‹œ: load_dataset(\"json\", data_files=\"your_file.jsonl\")\n",
        "\n",
        "print(f\"ğŸ“š Loading: {DATASET_REPO} ({DATASET_CONFIG})\")\n",
        "dataset = load_dataset(DATASET_REPO, DATASET_CONFIG, split=\"train\")\n",
        "print(f\"   Sample Count: {len(dataset)}\")\n",
        "\n",
        "def Format_Bidirectional_Prompt(example):\n",
        "    direction = example.get('direction', 'en2ko')\n",
        "\n",
        "    if direction == 'ko2en':\n",
        "        sys_msg = \"Translate the following text to English.\"\n",
        "    else:\n",
        "        sys_msg = \"Translate the following text to Korean.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_msg},\n",
        "        {\"role\": \"user\", \"content\": example['input']}\n",
        "    ]\n",
        "\n",
        "    prompt_text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt_text,\n",
        "        \"src\": example['input'],\n",
        "        \"ref\": example['output']\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(Format_Bidirectional_Prompt)\n",
        "print(f\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. GRPO í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "\n",
        "# T4ëŠ” FP16 ê°•ì œ\n",
        "if \"T4\" in gpu_name_short:\n",
        "    print(\"âš ï¸ T4 Detected: Forcing FP16\")\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"./grpo-output\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=2e-6,\n",
        "    \n",
        "    # GRPO ì„¤ì •\n",
        "    num_generations=4,\n",
        "    max_completion_length=200,\n",
        "    beta=0.04,\n",
        "    use_vllm=True,\n",
        "    vllm_gpu_memory_utilization=VLLM_MEM,\n",
        "    \n",
        "    # ì •ë°€ë„ (T4: FP16)\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    \n",
        "    # ë¡œê¹…\n",
        "    logging_steps=1,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    report_to=\"none\",\n",
        "    \n",
        "    # Hub ì—…ë¡œë“œ\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=OUTPUT_MODEL_ID,\n",
        "    hub_strategy=\"every_save\",\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=reward_function,\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ\n",
        "trainer.save_model(\"./final_model\")\n",
        "trainer.push_to_hub()\n",
        "print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {OUTPUT_MODEL_ID}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}