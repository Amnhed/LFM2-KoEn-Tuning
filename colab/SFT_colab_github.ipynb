{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ“ SFT v6.1: Colab Curriculum Learning\n",
                "\n",
                "**LFM2-1.2B ê¸°ë°˜ í•œêµ­ì–´-ì˜ì–´ ì–‘ë°©í–¥ ë²ˆì—­ SFT í•™ìŠµ**\n",
                "\n",
                "> **í™˜ê²½**: Google Colab T4 GPU\n",
                "\n",
                "## ì‚¬ìš© ì „ ì„¤ì •\n",
                "1. Colab Secretsì— `HF_TOKEN` ë“±ë¡\n",
                "2. `OUTPUT_MODEL_ID`ë¥¼ ë³¸ì¸ HuggingFace IDë¡œ ë³€ê²½\n",
                "\n",
                "## í•™ìŠµ ì „ëµ\n",
                "- **Base Model**: `gyung/lfm2-1.2b-koen-mt-v6-sft-200k` (Phase 1 ì™„ë£Œ)\n",
                "- **Dataset**: HuggingFace `gyung/lfm2-koen-samples`\n",
                "- **Method**: Curriculum Learning (ê³ í’ˆì§ˆ ë°ì´í„°ë¡œ ë¯¸ì„¸ì¡°ì •)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
                "!pip install -q transformers==4.54.0 trl>=0.18.2 peft>=0.15.2\n",
                "!pip install -q datasets accelerate bitsandbytes huggingface_hub\n",
                "\n",
                "print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ. ëŸ°íƒ€ì„ ì¬ì‹œì‘ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. HuggingFace ë¡œê·¸ì¸\n",
                "from huggingface_hub import login\n",
                "login()  # Colab Secrets ì‚¬ìš© ë˜ëŠ” ìˆ˜ë™ ì…ë ¥"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
                "import os\n",
                "os.environ['WANDB_DISABLED'] = 'true'\n",
                "os.environ['HF_HOME'] = '/tmp/hf_cache'  # ë””ìŠ¤í¬ ì ˆì•½\n",
                "\n",
                "import torch\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "from trl import SFTTrainer, SFTConfig, setup_chat_format\n",
                "\n",
                "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
                "print(f\"ğŸ”§ CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. ì„¤ì • (âš ï¸ OUTPUT_MODEL_IDëŠ” ë³¸ì¸ IDë¡œ ë³€ê²½!)\n",
                "MODEL_ID = \"gyung/lfm2-1.2b-koen-mt-v6-sft-200k\"  # Phase 1 ëª¨ë¸\n",
                "OUTPUT_MODEL_ID = \"YOUR_ID/your-sft-model\"  # âš ï¸ ë³€ê²½ í•„ìš”!\n",
                "DATASET_REPO = \"gyung/lfm2-koen-samples\"\n",
                "DATASET_CONFIG = \"manual_1000_sft\"  # SFTìš© ë°ì´í„°ì…‹\n",
                "\n",
                "print(f\"ğŸ“Œ Base Model: {MODEL_ID}\")\n",
                "print(f\"ğŸ“Œ Dataset: {DATASET_REPO} ({DATASET_CONFIG})\")\n",
                "print(f\"ğŸ“Œ Output: {OUTPUT_MODEL_ID}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. ëª¨ë¸ ë¡œë“œ\n",
                "print(\"ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "# Chat template ì„¤ì •\n",
                "if tokenizer.chat_template is None:\n",
                "    model, tokenizer = setup_chat_format(model, tokenizer)\n",
                "\n",
                "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {MODEL_ID}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. ë°ì´í„°ì…‹ ë¡œë“œ (HuggingFaceì—ì„œ ì§ì ‘)\n",
                "print(f\"ğŸ“š ë°ì´í„°ì…‹ ë¡œë”©: {DATASET_REPO} ({DATASET_CONFIG})\")\n",
                "dataset = load_dataset(DATASET_REPO, DATASET_CONFIG, split=\"train\")\n",
                "print(f\"âœ… ìƒ˜í”Œ ìˆ˜: {len(dataset)}\")\n",
                "\n",
                "# ë°ì´í„° í˜•ì‹ í™•ì¸\n",
                "print(\"\\nğŸ“‹ ìƒ˜í”Œ ì˜ˆì‹œ:\")\n",
                "print(dataset[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (Chat Format ë³€í™˜)\n",
                "def format_to_chat(example):\n",
                "    \"\"\"\n",
                "    SFT ë°ì´í„°ë¥¼ Chat Formatìœ¼ë¡œ ë³€í™˜\n",
                "    \n",
                "    ì›ë³¸ í˜•ì‹: {instruction, input, output}\n",
                "    ë³€í™˜ í˜•ì‹: {messages: [{role, content}, ...]}\n",
                "    \"\"\"\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": example['instruction']},\n",
                "        {\"role\": \"user\", \"content\": example['input']},\n",
                "        {\"role\": \"assistant\", \"content\": example['output']}\n",
                "    ]\n",
                "    return {\"messages\": messages}\n",
                "\n",
                "print(\"ğŸ”„ Chat Format ë³€í™˜ ì¤‘...\")\n",
                "train_dataset = dataset.map(format_to_chat)\n",
                "print(f\"âœ… ë³€í™˜ ì™„ë£Œ: {len(train_dataset)} samples\")\n",
                "\n",
                "# ë³€í™˜ ê²°ê³¼ í™•ì¸\n",
                "print(\"\\nğŸ“‹ ë³€í™˜ëœ ìƒ˜í”Œ ì˜ˆì‹œ:\")\n",
                "print(train_dataset[0]['messages'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. SFT í•™ìŠµ ì„¤ì •\n",
                "sft_config = SFTConfig(\n",
                "    output_dir=\"./sft-output\",\n",
                "    \n",
                "    # í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
                "    num_train_epochs=1,\n",
                "    per_device_train_batch_size=1,\n",
                "    gradient_accumulation_steps=16,\n",
                "    gradient_checkpointing=True,\n",
                "    \n",
                "    # ìµœì í™”\n",
                "    learning_rate=5e-6,  # ë‚®ì€ LR (Curriculum Learning)\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    warmup_ratio=0.05,\n",
                "    optim=\"paged_adamw_8bit\",\n",
                "    \n",
                "    # ì •ë°€ë„ (T4: FP16)\n",
                "    fp16=True,\n",
                "    bf16=False,\n",
                "    \n",
                "    # ë¡œê¹… ë° ì €ì¥\n",
                "    logging_steps=50,\n",
                "    save_strategy=\"steps\",\n",
                "    save_steps=500,\n",
                "    save_total_limit=2,\n",
                "    \n",
                "    # Hub ì—…ë¡œë“œ\n",
                "    push_to_hub=True,\n",
                "    hub_model_id=OUTPUT_MODEL_ID,\n",
                "    hub_strategy=\"every_save\",\n",
                "    \n",
                "    # ë°ì´í„°ì…‹ ì„¤ì •\n",
                "    dataset_text_field=\"messages\",\n",
                "    packing=False,\n",
                "    report_to=\"none\",\n",
                ")\n",
                "\n",
                "print(\"âœ… í•™ìŠµ ì„¤ì • ì™„ë£Œ\")\n",
                "print(f\"   Learning Rate: {sft_config.learning_rate}\")\n",
                "print(f\"   Batch Size: {sft_config.per_device_train_batch_size} x {sft_config.gradient_accumulation_steps} = {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Trainer ìƒì„±\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=train_dataset,\n",
                "    args=sft_config,\n",
                "    processing_class=tokenizer,\n",
                ")\n",
                "\n",
                "print(\"âœ… Trainer ìƒì„± ì™„ë£Œ\")\n",
                "print(f\"   ì˜ˆìƒ í•™ìŠµ Step ìˆ˜: {len(train_dataset) // (sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. í•™ìŠµ ì‹œì‘!\n",
                "print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "trainer.train()\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 11. ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ\n",
                "print(\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ ì¤‘...\")\n",
                "\n",
                "trainer.save_model(\"./final_model\")\n",
                "tokenizer.save_pretrained(\"./final_model\")\n",
                "\n",
                "trainer.push_to_hub()\n",
                "\n",
                "print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: https://huggingface.co/{OUTPUT_MODEL_ID}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 12. (ì„ íƒ) í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
                "print(\"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸...\")\n",
                "\n",
                "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥\n",
                "test_texts = [\n",
                "    \"Please fix this error.\",\n",
                "    \"The model is working correctly now.\",\n",
                "    \"Time flies like an arrow.\",\n",
                "]\n",
                "\n",
                "for text in test_texts:\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": \"Translate to Korean.\"},\n",
                "        {\"role\": \"user\", \"content\": text}\n",
                "    ]\n",
                "    \n",
                "    inputs = tokenizer.apply_chat_template(\n",
                "        messages, \n",
                "        return_tensors=\"pt\", \n",
                "        add_generation_prompt=True\n",
                "    ).to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            inputs,\n",
                "            max_new_tokens=128,\n",
                "            do_sample=True,\n",
                "            temperature=0.3,\n",
                "        )\n",
                "    \n",
                "    result = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
                "    print(f\"ğŸ“ EN: {text}\")\n",
                "    print(f\"ğŸ‡°ğŸ‡· KO: {result}\")\n",
                "    print(\"-\" * 40)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}