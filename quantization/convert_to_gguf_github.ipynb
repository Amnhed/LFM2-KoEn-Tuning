{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ—ï¸ LFM2-1.2B GGUF ë³€í™˜ ë° ì–‘ìí™”\n",
        "\n",
        "HuggingFace ëª¨ë¸ì„ GGUFë¡œ ë³€í™˜í•˜ê³  ì–‘ìí™”í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ì§€ì› ì–‘ìí™”\n",
        "- Q8_0: ìµœê³  í’ˆì§ˆ\n",
        "- Q5_K_M: ê· í˜• (ì¶”ì²œ)\n",
        "- Q4_K_M: ê²½ëŸ‰í™”\n",
        "\n",
        "## í™˜ê²½\n",
        "- Colab CPUë¡œ ì¶©ë¶„ (GPU ë¶ˆí•„ìš”)\n",
        "- High-RAM ê¶Œì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. llama.cpp ë¹Œë“œ\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd /content/llama.cpp\n",
        "!mkdir build && cd build && cmake .. && cmake --build . --config Release\n",
        "%cd /content/llama.cpp\n",
        "!pip install -r requirements.txt\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. HuggingFace ë¡œê·¸ì¸\n",
        "from huggingface_hub import login\n",
        "login()  # ìˆ˜ë™ ì…ë ¥ ë˜ëŠ” Colab Secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. ì„¤ì • (ë³¸ì¸ IDë¡œ ë³€ê²½)\n",
        "MODEL_ID = \"gyung/lfm2-1.2b-koen-mt-v8-rl-10k-merged\"  # ë³€í™˜í•  ëª¨ë¸\n",
        "OUTPUT_REPO = \"YOUR_ID/your-model-GGUF\"                 # ì¶œë ¥ ë ˆí¬ (ë³€ê²½ í•„ìš”)\n",
        "MODEL_NAME = \"lfm2-1.2b-koen-mt-v8\"                     # íŒŒì¼ëª… prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "local_path = f\"/content/{MODEL_NAME}\"\n",
        "snapshot_download(repo_id=MODEL_ID, local_dir=local_path)\n",
        "print(f\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. FP16 GGUF ë³€í™˜\n",
        "%cd /content/llama.cpp\n",
        "\n",
        "!python convert_hf_to_gguf.py {local_path} \\\n",
        "    --outfile /content/{MODEL_NAME}-fp16.gguf \\\n",
        "    --outtype f16\n",
        "\n",
        "print(\"âœ… FP16 GGUF ë³€í™˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. ì–‘ìí™”\n",
        "QUANTIZATIONS = [\"Q8_0\", \"Q5_K_M\", \"Q4_K_M\"]\n",
        "\n",
        "for quant in QUANTIZATIONS:\n",
        "    print(f\"ğŸ”„ {quant} ì–‘ìí™” ì¤‘...\")\n",
        "    !./build/bin/llama-quantize \\\n",
        "        /content/{MODEL_NAME}-fp16.gguf \\\n",
        "        /content/{MODEL_NAME}-{quant}.gguf \\\n",
        "        {quant}\n",
        "    print(f\"âœ… {quant} ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. HuggingFace ì—…ë¡œë“œ\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "api = HfApi()\n",
        "create_repo(OUTPUT_REPO, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# ëª¨ë“  GGUF íŒŒì¼ ì—…ë¡œë“œ\n",
        "import glob\n",
        "for gguf_file in glob.glob(f\"/content/{MODEL_NAME}*.gguf\"):\n",
        "    print(f\"ğŸ“¤ ì—…ë¡œë“œ: {gguf_file}\")\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=gguf_file,\n",
        "        path_in_repo=gguf_file.split(\"/\")[-1],\n",
        "        repo_id=OUTPUT_REPO,\n",
        "        repo_type=\"model\",\n",
        "    )\n",
        "\n",
        "print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: https://huggingface.co/{OUTPUT_REPO}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}