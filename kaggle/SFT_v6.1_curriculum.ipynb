{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFM2-1.2B v6.1 Curriculum Learning (Phase 2)\n",
    "\n",
    "**Phase 2: High Quality Curriculum Learning**\n",
    "1. **Base Model**: `gyung/lfm2-1.2b-koen-mt-v6-sft-200k` (Phase 1 ì™„ë£Œ, Step 12504, Loss 1.3220)\n",
    "2. **Dataset**: `lfm2_koen_v6_sft_200k_filtered.jsonl` (CometKiwi > 0.85, 80,611ê°œ)\n",
    "3. **Training**: 1 epoch on high quality data with lower learning rate (5e-6)\n",
    "4. **Output**: `gyung/lfm2-1.2b-koen-mt-v6.1-curriculum`\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŸ Kaggle ì„¤ì • (í•„ìˆ˜)\n",
    "1. **Accelerator**: **GPU T4 x 2**\n",
    "2. **Internet**: **On**\n",
    "3. **Secrets**: `HF_TOKEN` ì„¤ì •\n",
    "4. **Dataset**: `lfm2_koen_v6_sft_200k_filtered.jsonl` ì—…ë¡œë“œ í•„ìš”\n",
    "\n",
    "### â±ï¸ ì˜ˆìƒ ì‹œê°„\n",
    "- ~5000 steps (~4-5ì‹œê°„)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (/tmp ì‚¬ìš© - ë””ìŠ¤í¬ ì ˆì•½)\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/tmp/hf_cache/datasets\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/tmp/hf_cache/transformers\"\n",
    "\n",
    "# 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -q transformers==4.54.0 trl>=0.18.2 peft>=0.15.2 datasets accelerate bitsandbytes huggingface_hub protobuf==3.20.3\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# 3. Hugging Face í† í°\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    os.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    print(\"âœ… HF_TOKEN í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ í† í° ì„¤ì • ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile train_phase2_curriculum.py\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainerCallback\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# ===========================================\n",
    "# Custom Callback: Explicit Commit Message\n",
    "# ===========================================\n",
    "class CurriculumCallback(TrainerCallback):\n",
    "    \"\"\"ëª…ì‹œì  commit messageë¡œ ì—…ë¡œë“œ (DDP: rank 0ë§Œ)\"\"\"\n",
    "    def __init__(self, hf_token, hub_model_id):\n",
    "        self.hf_token = hf_token\n",
    "        self.hub_model_id = hub_model_id\n",
    "        self.api = HfApi(token=hf_token)\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"ë¡œê·¸ ì¶œë ¥\"\"\"\n",
    "        if logs and 'loss' in logs:\n",
    "            print(f\"ğŸ“Š [Step {state.global_step}] loss={logs.get('loss', 'N/A'):.4f}\")\n",
    "    \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        \"\"\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œ ì—…ë¡œë“œ (DDP: rank 0ë§Œ)\"\"\"\n",
    "        local_rank = getattr(args, 'local_rank', -1)\n",
    "        if local_rank not in [-1, 0]:\n",
    "            return control\n",
    "        \n",
    "        checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "        \n",
    "        if os.path.exists(checkpoint_dir):\n",
    "            loss_val = 'N/A'\n",
    "            if state.log_history:\n",
    "                for log in reversed(state.log_history):\n",
    "                    if 'loss' in log:\n",
    "                        loss_val = f\"{log['loss']:.4f}\"\n",
    "                        break\n",
    "            commit_msg = f\"Step {state.global_step} checkpoint (loss: {loss_val})\"\n",
    "            print(f\"\\nğŸš€ Uploading with commit: '{commit_msg}'\")\n",
    "            try:\n",
    "                self.api.upload_folder(\n",
    "                    folder_path=checkpoint_dir,\n",
    "                    repo_id=self.hub_model_id,\n",
    "                    commit_message=commit_msg,\n",
    "                    repo_type=\"model\"\n",
    "                )\n",
    "                print(f\"âœ… Uploaded Step {state.global_step}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Upload failed: {e}\")\n",
    "        return control\n",
    "\n",
    "def find_dataset(target_files):\n",
    "    for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "        for target in target_files:\n",
    "            if target in files:\n",
    "                return os.path.join(root, target)\n",
    "    return None\n",
    "\n",
    "def create_mt_dataset(file_path):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ìƒì„± (ì–‘ë°©í–¥ ë²ˆì—­)\"\"\"\n",
    "    print(f\"ğŸ”„ Loading dataset from {file_path}...\")\n",
    "    base_dataset = load_dataset(\"json\", data_files=file_path, split=\"train\")\n",
    "    \n",
    "    def format_en2ko(row):\n",
    "        return {\"messages\": [{\"role\":\"system\",\"content\":\"Translate to Korean.\"}, \n",
    "                            {\"role\":\"user\",\"content\":row['input']}, \n",
    "                            {\"role\":\"assistant\",\"content\":row['output']}]}\n",
    "    def format_ko2en(row):\n",
    "        return {\"messages\": [{\"role\":\"system\",\"content\":\"Translate to English.\"}, \n",
    "                            {\"role\":\"user\",\"content\":row['output']}, \n",
    "                            {\"role\":\"assistant\",\"content\":row['input']}]}\n",
    "    \n",
    "    ds_en2ko = base_dataset.map(format_en2ko, remove_columns=base_dataset.column_names, num_proc=4)\n",
    "    ds_ko2en = base_dataset.map(format_ko2en, remove_columns=base_dataset.column_names, num_proc=4)\n",
    "    \n",
    "    full_dataset = concatenate_datasets([ds_en2ko, ds_ko2en])\n",
    "    full_dataset = full_dataset.shuffle(seed=42)\n",
    "    \n",
    "    print(f\"ğŸ“Š Total Dataset Size: {len(full_dataset):,}\")\n",
    "    return full_dataset\n",
    "\n",
    "def cleanup_gpu():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"ğŸ§¹ GPU Memory Cleaned\")\n",
    "\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"ğŸ§¹ Cleaning up: {output_dir}\")\n",
    "        shutil.rmtree(output_dir, ignore_errors=True)\n",
    "        print(\"âœ… Disk space freed\")\n",
    "\n",
    "# ==========================================\n",
    "# Phase 2: Curriculum Learning (80k HQ)\n",
    "# ==========================================\n",
    "def train_phase2_curriculum(hf_token):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“ PHASE 2: High Quality Curriculum Learning (1k HQ)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    PHASE1_MODEL_ID = \"gyung/lfm2-1.2b-koen-mt-v6-sft-200k\"  # Phase 1 ì™„ë£Œ ëª¨ë¸\n",
    "    DATASET_REPO = \"gyung/lfm2-koen-samples\"\n",
    "    DATASET_CONFIG = \"manual_1000_sft\"    # 1k ê³ í’ˆì§ˆ ë°ì´í„°\n",
    "    OUTPUT_DIR = \"/kaggle/working/phase2_curriculum\"\n",
    "    HUB_MODEL_ID = \"gyung/lfm2-1.2b-koen-mt-v6.1-curriculum\"\n",
    "    \n",
    "    dataset_path = find_dataset([DATASET_FILE])\n",
    "    if not dataset_path:\n",
    "        print(f\"âŒ Dataset not found: {DATASET_FILE}\")\n",
    "        return None\n",
    "    \n",
    "    # 1. Phase 1 ëª¨ë¸ ë¡œë“œ\n",
    "    print(f\"ğŸ“¦ Loading Phase 1 Model: {PHASE1_MODEL_ID}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PHASE1_MODEL_ID)\n",
    "    model = AutoModelForCausalLM.from_pretrained(PHASE1_MODEL_ID, torch_dtype=\"float16\")\n",
    "    \n",
    "    if tokenizer.chat_template is None:\n",
    "        model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "    \n",
    "    # 2. ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "    print(f\"ğŸ“š Loading: {DATASET_REPO} ({DATASET_CONFIG})\")\n",
    "                train_dataset = load_dataset(DATASET_REPO, DATASET_CONFIG, split=\"train\")\n",
    "    \n",
    "    # 3. í•™ìŠµ ì„¤ì • (ë‚®ì€ LRë¡œ Fine-tuning)\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        hub_model_id=HUB_MODEL_ID,\n",
    "        hub_token=hf_token,\n",
    "        push_to_hub=False,  # Callbackì—ì„œ ì²˜ë¦¬\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        learning_rate=5e-6,  # ë‚®ì€ LR\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        logging_steps=250,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        dataset_text_field=\"messages\",\n",
    "        packing=False,\n",
    "        report_to=\"none\",\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "    \n",
    "    # 4. Callback ì¶”ê°€\n",
    "    curriculum_callback = CurriculumCallback(\n",
    "        hf_token=hf_token,\n",
    "        hub_model_id=HUB_MODEL_ID\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        args=sft_config,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[curriculum_callback],\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸš€ Starting Phase 2 Training...\")\n",
    "    print(f\"   Learning Rate: 5e-6 (Lower for curriculum)\")\n",
    "    print(f\"   Dataset: {len(train_dataset):,} samples\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # 5. ìµœì¢… ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ\n",
    "    print(\"ğŸ’¾ Phase 2: Final Saving & Uploading...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    # DDP rank 0ë§Œ ìµœì¢… ì—…ë¡œë“œ\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', -1))\n",
    "    if local_rank in [-1, 0]:\n",
    "        api = HfApi(token=hf_token)\n",
    "        final_step = trainer.state.global_step\n",
    "        try:\n",
    "            # README.md ìƒì„± (ì—…ë¡œë“œ ì˜¤ë¥˜ ë°©ì§€)\n",
    "            readme_path = os.path.join(OUTPUT_DIR, \"README.md\")\n",
    "            with open(readme_path, \"w\") as f:\n",
    "                f.write(f\"# LFM2-1.2B-KoEn-MT v6.1 Curriculum\\\\n\\\\n\")\n",
    "                f.write(f\"Fine-tuned on 80k high-quality EN-KO translation pairs.\\\\n\\\\n\")\n",
    "                f.write(f\"Base: gyung/lfm2-1.2b-koen-mt-v6-sft-200k\\\\n\")\n",
    "                f.write(f\"Final Step: {final_step}\\\\n\")\n",
    "            api.upload_folder(\n",
    "                folder_path=OUTPUT_DIR,\n",
    "                repo_id=HUB_MODEL_ID,\n",
    "                commit_message=f\"v6.1 Curriculum Complete - Step {final_step} (80k HQ Final)\",\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "            print(f\"âœ… Phase 2 Complete! Model: {HUB_MODEL_ID}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Final upload failed: {e}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del trainer, model\n",
    "    cleanup_gpu()\n",
    "    \n",
    "    return HUB_MODEL_ID\n",
    "\n",
    "# ==========================================\n",
    "# Main Execution\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“ LFM2-v6.1 Curriculum Learning (Phase 2)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if not hf_token:\n",
    "        raise ValueError(\"âŒ HF_TOKEN not found!\")\n",
    "    \n",
    "    login(token=hf_token)\n",
    "    print(\"âœ… Logged in to Hugging Face Hub\")\n",
    "    \n",
    "    # GPU ì •ë³´\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"ğŸ–¥ï¸ GPU Count: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Phase 2 ì‹¤í–‰\n",
    "    final_model = train_phase2_curriculum(hf_token)\n",
    "    \n",
    "    if final_model:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ‰ CURRICULUM LEARNING COMPLETE!\")\n",
    "        print(f\"ğŸ“¦ Final Model: {final_model}\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\nâŒ Training failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPU ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# DDPë¡œ í•™ìŠµ ì‹¤í–‰\n",
    "!accelerate launch --multi_gpu --num_processes=2 train_phase2_curriculum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ğŸ§ª í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"gyung/lfm2-1.2b-koen-mt-v6.1-curriculum\"\n",
    "\n",
    "print(f\"ğŸ”¹ Loading: {MODEL_ID}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=\"float16\", device_map=\"auto\")\n",
    "\n",
    "def translate(text, to=\"ko\"):\n",
    "    system = \"Translate to Korean.\" if to == \"ko\" else \"Translate to English.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": text}]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            min_p=0.15,\n",
    "            repetition_penalty=1.05,\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "tests = [\n",
    "    \"Please fix this error.\",\n",
    "    \"The model is working correctly now.\",\n",
    "    \"Time flies like an arrow.\",\n",
    "]\n",
    "\n",
    "for text in tests:\n",
    "    result = translate(text, to=\"ko\")\n",
    "    print(f\"ğŸ“ EN: {text}\")\n",
    "    print(f\"ğŸ‡°ğŸ‡· KO: {result}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
