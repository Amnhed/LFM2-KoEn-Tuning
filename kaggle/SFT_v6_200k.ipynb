{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 14234714,
     "sourceType": "datasetVersion",
     "datasetId": 9081441
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LFM2-1.2B v6 SFT Training (200k + 80k Curriculum) - FIXED\n",
    "\n",
    "**v6 Training Pipeline**:\n",
    "1. **Phase 1**: 200k ì „ì²´ SFT (Base â†’ v6-SFT-200k)\n",
    "2. **Phase 2**: 80k ê³ í’ˆì§ˆ SFT (v6-SFT-200k â†’ v6.1-SFT-80k)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ ë””ìŠ¤í¬ ìµœì í™” (FIXED)\n",
    "1. **ìºì‹œë¥¼ /tmpë¡œ ì´ë™**: `/kaggle/working`ì€ 20GB ì œí•œ! â†’ `/tmp`ëŠ” ë³„ë„ ìš©ëŸ‰\n",
    "2. **ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹ˆë„ ê°ì†Œ**: 500 â†’ **2000** steps\n",
    "3. **ì²´í¬í¬ì¸íŠ¸ ìˆ˜ ì œí•œ**: 3 â†’ 2ê°œ\n",
    "\n",
    "### ğŸŒŸ Kaggle ì„¤ì • (í•„ìˆ˜)\n",
    "1. **Accelerator**: **GPU T4 x 2**\n",
    "2. **Internet**: **On**\n",
    "3. **Secrets**: `HF_TOKEN` ì„¤ì • (Hugging Face Write ê¶Œí•œ í† í°)\n",
    "4. **Dataset**: HuggingFace `gyung/lfm2-koen-samples` (ìë™ ë‹¤ìš´ë¡œë“œ)\n",
    "\n",
    "### â±ï¸ ì˜ˆìƒ ì‹œê°„\n",
    "- Phase 1 (200k): ~8-10ì‹œê°„\n",
    "- Phase 2 (80k): ~3-4ì‹œê°„\n",
    "- **ì´: ~12-14ì‹œê°„**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 1. ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (/tmp ì‚¬ìš© - ë””ìŠ¤í¬ ì ˆì•½ í•µì‹¬!)\nimport os\nos.environ[\"HF_HOME\"] = \"/tmp/hf_cache\"\nos.environ[\"HF_DATASETS_CACHE\"] = \"/tmp/hf_cache/datasets\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"/tmp/hf_cache/transformers\"\n\n# 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n!pip install -q transformers==4.54.0 trl>=0.18.2 peft>=0.15.2 datasets accelerate bitsandbytes huggingface_hub protobuf==3.20.3\n\nfrom kaggle_secrets import UserSecretsClient\n\n# 3. Hugging Face í† í° ê°€ì ¸ì˜¤ê¸°\ntry:\n    user_secrets = UserSecretsClient()\n    os.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\n    print(\"âœ… HF_TOKEN í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ!\")\nexcept Exception as e:\n    print(f\"âš ï¸ í† í° ì„¤ì • ì‹¤íŒ¨: {e}\")\n    print(\"Add-ons -> Secrets ì—ì„œ HF_TOKENì„ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile train_v6_fixed.py\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from trl import SFTTrainer, SFTConfig, setup_chat_format\n",
    "from huggingface_hub import login\n",
    "\n",
    "def find_dataset(target_files):\n",
    "    \"\"\"Kaggle inputì—ì„œ ë°ì´í„°ì…‹ íŒŒì¼ ì°¾ê¸°\"\"\"\n",
    "    for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "        for target in target_files:\n",
    "            if target in files:\n",
    "                return os.path.join(root, target)\n",
    "    return None\n",
    "\n",
    "def create_mt_dataset(repo_id, config_name):\n",
    "    \"\"\"HuggingFaceì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ\"\"\"\n",
    "    print(f\"ğŸ“š Loading: {repo_id} ({config_name})\")\n",
    "    base_dataset = load_dataset(repo_id, config_name, split=\"train\")\n",
    "    \n",
    "    def format_en2ko(row):\n",
    "        return {\"messages\": [{\"role\":\"system\",\"content\":\"Translate to Korean.\"}, \n",
    "                            {\"role\":\"user\",\"content\":row['input']}, \n",
    "                            {\"role\":\"assistant\",\"content\":row['output']}]}\n",
    "    def format_ko2en(row):\n",
    "        return {\"messages\": [{\"role\":\"system\",\"content\":\"Translate to English.\"}, \n",
    "                            {\"role\":\"user\",\"content\":row['output']}, \n",
    "                            {\"role\":\"assistant\",\"content\":row['input']}]}\n",
    "    \n",
    "    ds_en2ko = base_dataset.map(format_en2ko, remove_columns=base_dataset.column_names, num_proc=4)\n",
    "    ds_ko2en = base_dataset.map(format_ko2en, remove_columns=base_dataset.column_names, num_proc=4)\n",
    "    full_dataset = concatenate_datasets([ds_en2ko, ds_ko2en])\n",
    "    return full_dataset.shuffle(seed=42)\n",
    "\n",
    "def cleanup_checkpoints(output_dir):\n",
    "    \"\"\"ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œí•˜ì—¬ ë””ìŠ¤í¬ ê³µê°„ í™•ë³´\"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"ğŸ§¹ Cleaning up: {output_dir}\")\n",
    "        shutil.rmtree(output_dir, ignore_errors=True)\n",
    "\n",
    "def train_phase(model_id, dataset_repo, dataset_config, output_name, hub_model_id, hf_token, phase_name):\n",
    "    \"\"\"í•œ Phase í•™ìŠµ ì‹¤í–‰\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸš€ {phase_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ğŸ“¦ Model: {model_id}\")\n",
    "    print(f\"ğŸ“Š Dataset: {dataset_repo} ({dataset_config})\")\n",
    "    print(f\"ğŸ’¾ Output: {hub_model_id}\")\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    print(\"\\nğŸ”¹ Loading Model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\")\n",
    "    \n",
    "    if tokenizer.chat_template is None:\n",
    "        model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "    print(\"ğŸ”¹ Preparing Dataset...\")\n",
    "    train_dataset = create_mt_dataset(dataset_repo, dataset_config)\n",
    "    print(f\"   Total samples: {len(train_dataset):,}\")\n",
    "    \n",
    "    # í•™ìŠµ ì„¤ì • (ë””ìŠ¤í¬ ìµœì í™”)\n",
    "    output_dir = f\"/kaggle/working/{output_name}\"\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        hub_model_id=hub_model_id,\n",
    "        hub_token=hf_token,\n",
    "        push_to_hub=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        logging_steps=250,          # ë¡œê·¸ëŠ” 250 stepë§ˆë‹¤\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=1000,             # âœ… 1000 stepë§ˆë‹¤ ì €ì¥ (ë””ìŠ¤í¬ ì ˆì•½)\n",
    "        save_total_limit=2,          # âœ… ìµœëŒ€ 2ê°œë§Œ ìœ ì§€\n",
    "        eval_strategy=\"no\",          # í‰ê°€ ìƒëµ (ì†ë„)\n",
    "        dataset_text_field=\"messages\",\n",
    "        packing=False,\n",
    "        report_to=\"none\",\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        args=sft_config,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸš€ Starting Training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    if trainer.is_world_process_zero():\n",
    "        print(\"ğŸ’¾ Final Saving & Uploading to Hub...\")\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        trainer.push_to_hub()\n",
    "        try:\n",
    "            tokenizer.push_to_hub(hub_model_id)\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"âœ… {phase_name} Complete!\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # ë””ìŠ¤í¬ ì •ë¦¬ (Phase ì™„ë£Œ í›„)\n",
    "    cleanup_checkpoints(output_dir)\n",
    "    \n",
    "    return hub_model_id\n",
    "\n",
    "def main():\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "    else:\n",
    "        print(\"âš ï¸ HF_TOKEN not found. Uploads will fail.\")\n",
    "        return\n",
    "    \n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    \n",
    "    # ============================================\n",
    "    # âš™ï¸ ì„¤ì • (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ë©´ ë¨)\n",
    "    # ============================================\n",
    "    BASE_MODEL = \"LiquidAI/LFM2-1.2B\"\n",
    "    HUB_USERNAME = \"gyung\"\n",
    "    \n",
    "    # HuggingFace ë°ì´í„°ì…‹ ì„¤ì •\n",
    "    DATASET_REPO = \"gyung/lfm2-koen-samples\"\n",
    "    PHASE1_CONFIG = \"manual_1000_sft\"\n",
    "    PHASE1_OUTPUT = \"lfm2-1.2b-koen-mt-v6-sft-200k\"\n",
    "    \n",
    "    PHASE2_CONFIG = \"manual_1000_sft\"  # ê°™ì€ ë°ì´í„°ì…‹ ì‚¬ìš©\n",
    "    PHASE2_OUTPUT = \"lfm2-1.2b-koen-mt-v6.1-sft-80k\"\n",
    "    # ============================================\n",
    "    \n",
    "    # HuggingFaceì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ\n",
    "    print(f\"ğŸ“š Dataset: {DATASET_REPO}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ¯ v6 SFT Training Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Phase 1: {DATASET_REPO}/{PHASE1_CONFIG} â†’ {PHASE1_OUTPUT}\")\n",
    "    print(f\"Phase 2: {DATASET_REPO}/{PHASE2_CONFIG} â†’ {PHASE2_OUTPUT}\")\n",
    "    \n",
    "    # Phase 1: 200k í•™ìŠµ\n",
    "    phase1_model = train_phase(\n",
    "        model_id=BASE_MODEL,\n",
    "        dataset_repo=DATASET_REPO,\n",
    "        dataset_config=PHASE1_CONFIG,\n",
    "        output_name=PHASE1_OUTPUT,\n",
    "        hub_model_id=f\"{HUB_USERNAME}/{PHASE1_OUTPUT}\",\n",
    "        hf_token=hf_token,\n",
    "        phase_name=\"Phase 1: SFT 200k\"\n",
    "    )\n",
    "    \n",
    "    # Phase 2: 80k ê³ í’ˆì§ˆ í•™ìŠµ (Phase 1 ê²°ê³¼ì—ì„œ ì´ì–´ì„œ)\n",
    "    train_phase(\n",
    "        model_id=phase1_model,  # Phase 1 ê²°ê³¼ ëª¨ë¸ì—ì„œ ì‹œì‘\n",
    "        dataset_repo=DATASET_REPO,\n",
    "        dataset_config=PHASE2_CONFIG,\n",
    "        output_name=PHASE2_OUTPUT,\n",
    "        hub_model_id=f\"{HUB_USERNAME}/{PHASE2_OUTPUT}\",\n",
    "        hf_token=hf_token,\n",
    "        phase_name=\"Phase 2: SFT 80k (Curriculum)\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ‰ All Phases Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"âœ… v6-SFT-200k: https://huggingface.co/{HUB_USERNAME}/{PHASE1_OUTPUT}\")\n",
    "    print(f\"âœ… v6.1-SFT-80k: https://huggingface.co/{HUB_USERNAME}/{PHASE2_OUTPUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ğŸš€ í•™ìŠµ ì‹¤í–‰ (T4 x 2 DDP)\n!accelerate launch --multi_gpu --num_processes 2 --mixed_precision no train_v6_fixed.py",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# â›‘ï¸ Rescue Cell (í•™ìŠµ ì¤‘ë‹¨ ì‹œ ìˆ˜ë™ ì—…ë¡œë“œ)\nimport os\nfrom huggingface_hub import HfApi, login\nfrom kaggle_secrets import UserSecretsClient\n\n# ë¡œê·¸ì¸\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\n\n# ì—…ë¡œë“œí•  ëª¨ë¸ ì„ íƒ (ê²½ë¡œ í™•ì¸ í›„ ìˆ˜ì •)\nMODELS_TO_UPLOAD = {\n    \"/kaggle/working/lfm2-1.2b-koen-mt-v6-sft-200k\": \"gyung/lfm2-1.2b-koen-mt-v6-200k\",\n    \"/kaggle/working/lfm2-1.2b-koen-mt-v6.1-sft-80k\": \"gyung/lfm2-1.2b-koen-mt-v6.1-sft-80k\",\n}\n\napi = HfApi()\nfor local_path, repo_id in MODELS_TO_UPLOAD.items():\n    if os.path.exists(local_path):\n        print(f\"ğŸš€ Uploading {local_path} â†’ {repo_id}\")\n        try:\n            api.upload_folder(folder_path=local_path, repo_id=repo_id, repo_type=\"model\")\n            print(f\"âœ… Done: {repo_id}\")\n        except Exception as e:\n            print(f\"âŒ Failed: {e}\")\n    else:\n        print(f\"âš ï¸ Not found: {local_path}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ğŸ§ª í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì„ íƒ\nMODEL_ID = \"gyung/lfm2-1.2b-koen-mt-v6.1-sft-80k\"  # ìµœì¢… ëª¨ë¸\n\nprint(f\"ğŸ”¹ Loading: {MODEL_ID}\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=\"float16\", device_map=\"auto\")\n\ndef translate(text, to=\"ko\"):\n    system = \"Translate to Korean.\" if to == \"ko\" else \"Translate to English.\"\n    messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": text}]\n    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_new_tokens=256,\n            do_sample=True,\n            temperature=0.3,\n            min_p=0.15,\n            repetition_penalty=1.05,\n        )\n    return tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n\n# í…ŒìŠ¤íŠ¸\ntests = [\n    \"Please fix this error.\",\n    \"The model is working correctly now.\",\n    \"Time flies like an arrow.\",  # ê´€ìš©êµ¬ í…ŒìŠ¤íŠ¸\n]\n\nfor text in tests:\n    result = translate(text, to=\"ko\")\n    print(f\"ğŸ“ EN: {text}\")\n    print(f\"ğŸ‡°ğŸ‡· KO: {result}\")\n    print(\"-\" * 40)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}